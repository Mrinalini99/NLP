{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#17BCE0505 - WORDLIST, WORDNET, PIPELINE\n",
    "\n",
    "#Import the English stopwords from NLTK corpus\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133737\n",
      "('a', ['AH0'])\n",
      "('a.', ['EY1'])\n",
      "('a', ['EY1'])\n",
      "('a42128', ['EY1', 'F', 'AO1', 'R', 'T', 'UW1', 'W', 'AH1', 'N', 'T', 'UW1', 'EY1', 'T'])\n",
      "('aaa', ['T', 'R', 'IH2', 'P', 'AH0', 'L', 'EY1'])\n",
      "('aaberg', ['AA1', 'B', 'ER0', 'G'])\n",
      "('aachen', ['AA1', 'K', 'AH0', 'N'])\n",
      "('aachener', ['AA1', 'K', 'AH0', 'N', 'ER0'])\n",
      "('aaker', ['AA1', 'K', 'ER0'])\n",
      "('aalseth', ['AA1', 'L', 'S', 'EH0', 'TH'])\n",
      "('aamodt', ['AA1', 'M', 'AH0', 'T'])\n",
      "('aancor', ['AA1', 'N', 'K', 'AO2', 'R'])\n",
      "('aardema', ['AA0', 'R', 'D', 'EH1', 'M', 'AH0'])\n",
      "('aardvark', ['AA1', 'R', 'D', 'V', 'AA2', 'R', 'K'])\n",
      "('aaron', ['EH1', 'R', 'AH0', 'N'])\n",
      "(\"aaron's\", ['EH1', 'R', 'AH0', 'N', 'Z'])\n",
      "('aarons', ['EH1', 'R', 'AH0', 'N', 'Z'])\n",
      "('aaronson', ['EH1', 'R', 'AH0', 'N', 'S', 'AH0', 'N'])\n",
      "('aaronson', ['AA1', 'R', 'AH0', 'N', 'S', 'AH0', 'N'])\n",
      "(\"aaronson's\", ['EH1', 'R', 'AH0', 'N', 'S', 'AH0', 'N', 'Z'])\n",
      "(\"aaronson's\", ['AA1', 'R', 'AH0', 'N', 'S', 'AH0', 'N', 'Z'])\n",
      "('aarti', ['AA1', 'R', 'T', 'IY2'])\n",
      "('aase', ['AA1', 'S'])\n",
      "('aasen', ['AA1', 'S', 'AH0', 'N'])\n",
      "('ab', ['AE1', 'B'])\n",
      "('ab', ['EY1', 'B', 'IY1'])\n",
      "('ababa', ['AH0', 'B', 'AA1', 'B', 'AH0'])\n",
      "('ababa', ['AA1', 'B', 'AH0', 'B', 'AH0'])\n",
      "('abacha', ['AE1', 'B', 'AH0', 'K', 'AH0'])\n",
      "('aback', ['AH0', 'B', 'AE1', 'K'])\n",
      "('abaco', ['AE1', 'B', 'AH0', 'K', 'OW2'])\n",
      "('abacus', ['AE1', 'B', 'AH0', 'K', 'AH0', 'S'])\n",
      "('abad', ['AH0', 'B', 'AA1', 'D'])\n",
      "('abadaka', ['AH0', 'B', 'AE1', 'D', 'AH0', 'K', 'AH0'])\n",
      "('abadi', ['AH0', 'B', 'AE1', 'D', 'IY0'])\n",
      "('abadie', ['AH0', 'B', 'AE1', 'D', 'IY0'])\n",
      "('abair', ['AH0', 'B', 'EH1', 'R'])\n",
      "('abalkin', ['AH0', 'B', 'AA1', 'L', 'K', 'IH0', 'N'])\n",
      "('abalone', ['AE2', 'B', 'AH0', 'L', 'OW1', 'N', 'IY0'])\n",
      "('abalos', ['AA0', 'B', 'AA1', 'L', 'OW0', 'Z'])\n",
      "('abandon', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N'])\n",
      "('abandoned', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'D'])\n",
      "('abandoning', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'IH0', 'NG'])\n",
      "('abandonment', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'M', 'AH0', 'N', 'T'])\n",
      "('abandonments', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'M', 'AH0', 'N', 'T', 'S'])\n",
      "('abandons', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'Z'])\n",
      "('abanto', ['AH0', 'B', 'AE1', 'N', 'T', 'OW0'])\n",
      "('abarca', ['AH0', 'B', 'AA1', 'R', 'K', 'AH0'])\n",
      "('abare', ['AA0', 'B', 'AA1', 'R', 'IY0'])\n",
      "('abascal', ['AE1', 'B', 'AH0', 'S', 'K', 'AH0', 'L'])\n"
     ]
    }
   ],
   "source": [
    "#Task No. 1\n",
    "\n",
    "#CMU Wordlist\n",
    "import nltk\n",
    "#Retrieve entries and print the no. of entries (length)\n",
    "entries=nltk.corpus.cmudict.entries()\n",
    "print(len(entries))\n",
    "#Print the length for each entry\n",
    "for entry in entries[:50]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to C:\\Users\\Anusha\n",
      "[nltk_data]     Sinha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Task No. 2\n",
    "\n",
    "#Wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "#Retrieve ids for the subsets\n",
    "wn.synsets('motorcar')\n",
    "#Print the head words/lemmas in the subset\n",
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Anusha\n",
      "[nltk_data]     Sinha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anusha Sinha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Heath', 'NNP'), ('Andrew', 'NNP'), ('Ledger', 'NNP'), ('(', '('), ('4', 'CD'), ('April', 'NNP'), ('1979', 'CD'), ('–', 'NNP'), ('22', 'CD'), ('January', 'NNP'), ('2008', 'CD'), (')', ')'), ('[', 'VBD'), ('1', 'CD'), (']', 'NN'), ('was', 'VBD'), ('an', 'DT'), ('Australian', 'JJ'), ('actor', 'NN'), ('and', 'CC'), ('music', 'NN'), ('video', 'NN'), ('director', 'NN'), ('.', '.')]\n",
      "[('After', 'IN'), ('performing', 'VBG'), ('roles', 'NNS'), ('in', 'IN'), ('several', 'JJ'), ('Australian', 'JJ'), ('television', 'NN'), ('and', 'CC'), ('film', 'NN'), ('productions', 'NNS'), ('during', 'IN'), ('the', 'DT'), ('1990s', 'CD'), (',', ','), ('Ledger', 'NNP'), ('left', 'VBD'), ('for', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('in', 'IN'), ('1998', 'CD'), ('to', 'TO'), ('further', 'RBR'), ('develop', 'VB'), ('his', 'PRP$'), ('film', 'NN'), ('career', 'NN'), ('.', '.')]\n",
      "[('His', 'PRP$'), ('work', 'NN'), ('comprised', 'VBD'), ('nineteen', 'JJ'), ('films', 'NNS'), (',', ','), ('including', 'VBG'), ('10', 'CD'), ('Things', 'NNS'), ('I', 'PRP'), ('Hate', 'VBP'), ('About', 'IN'), ('You', 'PRP'), ('(', '('), ('1999', 'CD'), (')', ')'), (',', ','), ('The', 'DT'), ('Patriot', 'NNP'), ('(', '('), ('2000', 'CD'), (')', ')'), (',', ','), ('A', 'NNP'), ('Knight', 'NNP'), (\"'s\", 'POS'), ('Tale', 'NNP'), ('(', '('), ('2001', 'CD'), (')', ')'), (',', ','), ('Monster', 'NNP'), (\"'s\", 'POS'), ('Ball', 'NNP'), ('(', '('), ('2001', 'CD'), (')', ')'), (',', ','), ('Lords', 'NNP'), ('of', 'IN'), ('Dogtown', 'NNP'), ('(', '('), ('2005', 'CD'), (')', ')'), (',', ','), ('Brokeback', 'NNP'), ('Mountain', 'NNP'), ('(', '('), ('2005', 'CD'), (')', ')'), (',', ','), ('The', 'DT'), ('Dark', 'NNP'), ('Knight', 'NNP'), ('(', '('), ('2008', 'CD'), (')', ')'), (',', ','), ('and', 'CC'), ('The', 'DT'), ('Imaginarium', 'NNP'), ('of', 'IN'), ('Doctor', 'NNP'), ('Parnassus', 'NNP'), ('(', '('), ('2009', 'CD'), (')', ')'), (',', ','), ('the', 'DT'), ('latter', 'JJ'), ('two', 'CD'), ('being', 'VBG'), ('posthumous', 'JJ'), ('releases', 'NNS'), ('.', '.')]\n",
      "[('[', 'RB'), ('2', 'CD'), (']', 'NN'), ('He', 'PRP'), ('also', 'RB'), ('produced', 'VBD'), ('and', 'CC'), ('directed', 'VBD'), ('music', 'NN'), ('videos', 'NN'), ('and', 'CC'), ('aspired', 'VBD'), ('to', 'TO'), ('be', 'VB'), ('a', 'DT'), ('film', 'NN'), ('director', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#Task No. 3\n",
    "\n",
    "#NLTK Pipeline\n",
    "#Initialising the texts list\n",
    "texts=[\"\"\" Heath Andrew Ledger (4 April 1979 – 22 January 2008)[1] was an Australian actor and music video director. After performing roles in several Australian television and film productions during the 1990s, Ledger left for the United States in 1998 to further develop his film career. His work comprised nineteen films, including 10 Things I Hate About You (1999), The Patriot (2000), A Knight's Tale (2001), Monster's Ball (2001), Lords of Dogtown (2005), Brokeback Mountain (2005), The Dark Knight (2008), and The Imaginarium of Doctor Parnassus (2009), the latter two being posthumous releases.[2] He also produced and directed music videos and aspired to be a film director.\"\"\"]\n",
    "#Creating sentence tokens for each sentence\n",
    "for text in texts:\n",
    "    sentences=nltk.sent_tokenize(text)\n",
    "    #Creating word tokens in every sentence, followed by applying POS tagging on them\n",
    "    for sentence in sentences:\n",
    "        words=nltk.word_tokenize(sentence)\n",
    "        tagged_words=nltk.pos_tag(words)\n",
    "        print(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Democrats',\n",
       " 'are',\n",
       " 'now',\n",
       " 'the',\n",
       " 'party',\n",
       " 'of',\n",
       " 'high',\n",
       " 'taxes',\n",
       " ',',\n",
       " 'high',\n",
       " 'crime',\n",
       " ',',\n",
       " 'open',\n",
       " 'borders',\n",
       " ',',\n",
       " 'late-term',\n",
       " 'abortion',\n",
       " ',',\n",
       " 'socialism',\n",
       " ',',\n",
       " 'and',\n",
       " 'blatant',\n",
       " 'corruption',\n",
       " '.',\n",
       " 'The',\n",
       " 'Republican',\n",
       " 'Party',\n",
       " 'is',\n",
       " 'the',\n",
       " 'party',\n",
       " 'of',\n",
       " 'the',\n",
       " 'American',\n",
       " 'Worker',\n",
       " ',',\n",
       " 'the',\n",
       " 'American',\n",
       " 'Family',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'American',\n",
       " 'Dream',\n",
       " '!',\n",
       " ':-)',\n",
       " '-',\n",
       " '#KAG2020']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization implementation\n",
    "\n",
    "#Twitter Aware Tokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer   #Importing the TweetTokenizer from NLTK.\n",
    "#Initiliasing the content variable\n",
    "content='Democrats are now the party of high taxes, high crime, open borders, late-term abortion, socialism, and blatant corruption. The Republican Party is the party of the American Worker, the American Family, and the American Dream!:-)- #KAG2020'\n",
    "#Initialize the Tweet Tokenizer\n",
    "twetkn=TweetTokenizer()\n",
    "#Tokenising the content\n",
    "twetkn.tokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
